{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ketaki Barde\\AnacondaLatest\\lib\\site-packages\\dask\\dataframe\\utils.py:13: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import classification_report,  f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold, KFold, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string\n",
    "from gensim.parsing.preprocessing import stem_text\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "import gensim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]\n",
    "\n",
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "    \n",
    "    \n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "text = Pipeline([\n",
    "                ('selector', TextSelector(key='text')),\n",
    "                ('tfidf', TfidfVectorizer( stop_words='english'))\n",
    "            ])\n",
    "\n",
    "text.fit_transform(X_train)\n",
    "\n",
    "followers =  Pipeline([\n",
    "                ('selector', NumberSelector(key='followers')),\n",
    "                ('standard', MinMaxScaler())\n",
    "            ])\n",
    "\n",
    "followers.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_model():\n",
    "    \n",
    "    def preprocess(self, X):\n",
    "        \n",
    "        print(\"\\n\\nPreprocess module\\n\\n\")\n",
    "        text_column = X['title'] + ' ' + X['location'] + ' ' + X['requirements'] + ' ' + X['description']\n",
    "        X['text'] = text_column\n",
    "        \n",
    "        '''text_list=[]\n",
    "        for i, row in X.iterrows():\n",
    "            text_list.append(row['text'])'''\n",
    "             \n",
    "        \n",
    "        '''stemmer = PorterStemmer()\n",
    "        def lemmatize_stemming(text):\n",
    "            return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))'''\n",
    "\n",
    "        def preprocessor(text):\n",
    "            result = []\n",
    "            for token in gensim.utils.simple_preprocess(text):\n",
    "                if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2:\n",
    "                    result.append(stem_text(token))\n",
    "                    \n",
    "            for token in gensim.utils.simple_preprocess(text):\n",
    "                if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2:\n",
    "                    result.append(preprocess_string(token))\n",
    "                    \n",
    "            for token in gensim.utils.simple_preprocess(text):\n",
    "                if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2:\n",
    "                    result.append(strip_punctuation(token))\n",
    "                    \n",
    "            for token in gensim.utils.simple_preprocess(text):\n",
    "                result.append(remove_stopwords(token))\n",
    "            \n",
    "            \n",
    "                \n",
    "            return result\n",
    "                   \n",
    "        X['text'].fillna('').astype(str).map(preprocessor)\n",
    "        print(X['text'] [:2])\n",
    "        \n",
    "        print(\"preprocessing over\")\n",
    "        \n",
    "        return X\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        '''text_column = X['title'] + ' ' + X['location'] + ' ' + X['requirements'] + ' ' + X['description']\n",
    "        X['text'] = text_column'''\n",
    "        \n",
    "        #PASS IST OF STR TOKENS. [\"CAT\", \"dog\"] not series\n",
    "        \n",
    "        \n",
    "        \n",
    "        text = Pipeline([\n",
    "                ('selector', TextSelector(key='text')),\n",
    "                ('tfidf', TfidfVectorizer(stop_words='english', norm='l2', use_idf=True,\n",
    "                                            smooth_idf=True, max_df=0.3, sublinear_tf=True, lowercase=False)) ])\n",
    "\n",
    "        XX= text.fit_transform(X)\n",
    "        \n",
    "        \n",
    "        other1 =  Pipeline([\n",
    "                ('selector', NumberSelector(key='has_questions')),\n",
    "                ('no-transform', None )\n",
    "            ])\n",
    "\n",
    "        other1.fit_transform(X)\n",
    "        \n",
    "        other2 =  Pipeline([\n",
    "                ('selector', NumberSelector(key='has_company_logo')),\n",
    "                ('no-transform', None )\n",
    "            ])\n",
    "\n",
    "        other2.fit_transform(X)\n",
    "        \n",
    "        \n",
    "        other3 =  Pipeline([\n",
    "                ('selector', NumberSelector(key='telecommuting')),\n",
    "                ('no-transform', None )\n",
    "            ])\n",
    "\n",
    "        other3.fit_transform(X)\n",
    "        \n",
    "        \n",
    "        \n",
    "        feats = FeatureUnion([('text', text), \n",
    "                      ('questions', other1), ('logo', other2), ('telecommute', other3)])\n",
    "        \n",
    "        feature_processing = Pipeline([('feats', feats)])\n",
    "        feature_processing.fit_transform(X)\n",
    "\n",
    "        param_xgb={'subsample': 1.0, 'min_child_weight': 5, 'max_depth': 20, 'gamma': 0.5, 'colsample_bytree': 1.0, 'learning_rate' :0.02 }\n",
    "\n",
    "\n",
    "\n",
    "        estimators_stacking = [\n",
    "            \n",
    "                ('XGB', xgb.XGBClassifier(**param_xgb)),\n",
    "                \n",
    "            \n",
    "                ('MLPClassifier', MLPClassifier(\n",
    "                    hidden_layer_sizes=(3,),\n",
    "                    early_stopping=True,\n",
    "                    n_iter_no_change=5,\n",
    "                )),\n",
    "                ('KNeighborsClassifier', KNeighborsClassifier(\n",
    "                    n_neighbors=3,\n",
    "                    metric='cosine',\n",
    "                    n_jobs=3,\n",
    "                )),\n",
    "                ('RandomForestClassifier', RandomForestClassifier(\n",
    "                    n_estimators=125,\n",
    "                    min_samples_leaf=3,\n",
    "                    class_weight='balanced_subsample',\n",
    "                    n_jobs=3,\n",
    "                )),\n",
    "                ('SGDClassifier',SGDClassifier(\n",
    "                    alpha=1e-05,\n",
    "                    max_iter=500,\n",
    "                    tol=1e-4,\n",
    "                    learning_rate='adaptive',\n",
    "                    eta0=0.5,\n",
    "                    early_stopping=True,\n",
    "                    class_weight={1:0.8, 0:0.2},\n",
    "                )),\n",
    "            ]\n",
    "\n",
    "        clf = StackingClassifier(\n",
    "            estimators=estimators_stacking,\n",
    "            final_estimator=GradientBoostingClassifier(\n",
    "                subsample=0.75,\n",
    "                min_samples_leaf=3,\n",
    "                n_iter_no_change=5\n",
    "            )\n",
    "        )\n",
    "        #sgd = SGDClassifier( max_iter=3000, random_state=42, class_weight= {1:0.8, 0:0.2}, penalty='l2', loss='hinge', learning_rate='optimal', eta0=500)       \n",
    "           \n",
    "        self.pipeline = Pipeline([('features',feats),('classifier', clf )])\n",
    "\n",
    "        self.pipeline.fit(X, y)   \n",
    "        \n",
    "       \n",
    "        return \n",
    "\n",
    "    def predict(self, X):\n",
    "        # remember to apply the same preprocessing in fit() on test data before making predictions\n",
    "\n",
    "        text_column = X['title'] + ' ' + X['location'] + ' ' + X['requirements'] + ' ' + X['description']\n",
    "        X['text'] = text_column\n",
    "        \n",
    "        predictions =self.pipeline.predict(X)\n",
    "        \n",
    "        #predictions = self.clf.predict(featuresval)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Split ratio of Train: Validation :  (7152, 8) (1788, 8) \n",
      "\n",
      "\n",
      "Shape of Train X and Y : (7152, 7) (7152,)\n",
      "\n",
      "Shape of Train Split into train and test :  (5721, 7) (5721,) (1431, 7) (1431,)\n",
      "\n",
      "\n",
      "\n",
      "Shape of Validation X and Y : (1788, 7) (1788,)\n",
      "\n",
      "\n",
      "Preprocess module\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ketaki Barde\\AnacondaLatest\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5336    iOS Developer - Rithmio US, IL, Chicago Skills...\n",
      "6512      Manager of Project Management Organization -...\n",
      "Name: text, dtype: object\n",
      "preprocessing over\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    start = time.time()\n",
    "    # Load data\n",
    "    data = pd.read_csv(\"job_train.csv\")\n",
    "    \n",
    "    # Replace missing values with empty strings\n",
    "    data = data.fillna(\"\")\n",
    "\n",
    "        \n",
    "    def split_by_fractions(df:data, fracs:list, random_state:int=42):\n",
    "        assert sum(fracs)==1.0, 'fractions sum is not 1.0 (fractions_sum={})'.format(sum(fracs))\n",
    "        remain = df.index.copy().to_frame()\n",
    "        res = []\n",
    "        for i in range(len(fracs)):\n",
    "            fractions_sum=sum(fracs[i:])\n",
    "            frac = fracs[i]/fractions_sum\n",
    "            idxs = remain.sample(frac=frac, random_state=random_state).index\n",
    "            remain=remain.drop(idxs)\n",
    "            res.append(idxs)\n",
    "        return [df.loc[idxs] for idxs in res]\n",
    "    \n",
    "    train,val = split_by_fractions(data, [0.8,0.2]) # e.g: [ train, validation]\n",
    "\n",
    "    print(\"\\n\\nSplit ratio of Train: Validation : \",  train.shape, val.shape, \"\\n\")\n",
    "    \n",
    "    \n",
    "    # split the data\n",
    "    y=train[\"fraudulent\"]\n",
    "    X= train.drop(['fraudulent'], axis=1)\n",
    "    \n",
    "    print(\"\\nShape of Train X and Y :\", X.shape, y.shape)\n",
    "    \n",
    "   \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
    "    print(\"\\nShape of Train Split into train and test : \", X_train.shape, y_train.shape, X_test.shape, y_test.shape )\n",
    "    \n",
    "    \n",
    "    \n",
    "    yvaldn=val[\"fraudulent\"]\n",
    "    Xvaldn=val.drop(['fraudulent'], axis=1)\n",
    "    #Xvaldn['text'] = Xvaldn['title'] + ' ' + Xvaldn['location'] + ' ' + Xvaldn['requirements'] + ' ' + Xvaldn['description']\n",
    "    print(\"\\n\\n\\nShape of Validation X and Y :\", Xvaldn.shape, yvaldn.shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "    data_test = pd.read_csv(\"job_test.csv\")\n",
    "    data_test = data_test.fillna(\"\")\n",
    "    \n",
    "    ytest=data_test[\"fraudulent\"]\n",
    "    Xtest= data_test.drop(['fraudulent'], axis=1)\n",
    "    \n",
    "    clf = my_model()\n",
    "      \n",
    "        \n",
    "    X_train= clf.preprocess(X_train)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"fitted\")\n",
    "    \n",
    "    '''from the split train-test'''\n",
    "    #X_test = clf.preprocess(X_test)\n",
    "    pred = clf.predict(X_test)\n",
    "    #print(predictions[:3])\n",
    "    print(\"\\nF1 for Train: \", f1_score(y_test, pred))\n",
    "    \n",
    "    '''my custom split validation data --> unseen 20%'''  \n",
    "    #Xvaldn = clf.preprocess(Xvaldn)\n",
    "    predictions = clf.predict(Xvaldn)\n",
    "    #print(predictions[:3])\n",
    "    print(\"\\nF1 for Validation: \", f1_score(yvaldn, predictions))\n",
    "    \n",
    "    '''actual test set -- unseen given by prof , must be close to valdn'''\n",
    "    #Xtest = clf.preprocess(Xtest)\n",
    "    test_predictions = clf.predict(Xtest)\n",
    "    #print(predictions[:3])\n",
    "    print(\"\\nF1 for Test : \", f1_score(ytest, test_predictions))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    runtime = (time.time() - start) / 60.0\n",
    "    print(runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    8484\n",
       "1     456\n",
       "Name: fraudulent, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate majority and minority classes\n",
    "df=pd.read_csv(\"job_train.csv\")\n",
    "\n",
    "df_majority = df[df.fraudulent==0]\n",
    "df_minority = df[df.fraudulent==1]\n",
    "\n",
    "df.fraudulent.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
